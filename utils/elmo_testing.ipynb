{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datetime import date\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "sys.path.insert(0, '../')\n",
    "from models.elmo import returnElmoModel\n",
    "sys.path.insert(0, '../utils')\n",
    "from utils.allennlp_utils import returnDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7612it [00:04, 1640.91it/s]\n",
      "1632it [00:00, 1903.23it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = returnDataLoader(\"../data/train.csv\", 16)\n",
    "valid_loader = returnDataLoader(\"../data/valid.csv\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = returnElmoModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elmoModel(\n",
       "  (word_embeddings): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): ElmoTokenEmbedder(\n",
       "      (_elmo): Elmo(\n",
       "        (_elmo_lstm): _ElmoBiLm(\n",
       "          (_token_embedder): _ElmoCharacterEncoder(\n",
       "            (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "            (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "            (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "            (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "            (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "            (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "            (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "            (_highways): Highway(\n",
       "              (_layers): ModuleList(\n",
       "                (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "                (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (_elmo_lstm): ElmoLstm(\n",
       "            (forward_layer_0): LstmCellWithProjection(\n",
       "              (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "              (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "              (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "            )\n",
       "            (backward_layer_0): LstmCellWithProjection(\n",
       "              (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "              (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "              (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "            )\n",
       "            (forward_layer_1): LstmCellWithProjection(\n",
       "              (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "              (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "              (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "            )\n",
       "            (backward_layer_1): LstmCellWithProjection(\n",
       "              (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "              (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "              (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (_dropout): Dropout(p=0.5, inplace=False)\n",
       "        (scalar_mix_0): ScalarMix(\n",
       "          (scalar_parameters): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "              (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): PytorchSeq2VecWrapper(\n",
       "    (_module): LSTM(1024, 64, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (projection): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import get_cosine_with_hard_restarts_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_lr_decay(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return 1e-3\n",
    "        elif current_step < 2 * num_warmup_steps:\n",
    "            return 1e-2\n",
    "        return 1\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr = 3e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-5)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "#scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 30, # Default value in run_glue.py\n",
    "#                                            num_training_steps = len(train_loader.dataset))    \n",
    "#scheduler = special_lr_decay(optimizer, len(train_loader), num_training_steps = len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss 0.6856 - acc 0.5650 - lr 3e-05: 100%|██████████| 102/102 [00:11<00:00,  8.72it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_cosine_schedule_with_warmup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-02c43a1c889b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cosine_schedule_with_warmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_cosine_schedule_with_warmup' is not defined"
     ]
    }
   ],
   "source": [
    "#model = returnElmoModel()\n",
    "#model.to(device)\n",
    "#model.zero_grad()\n",
    "#model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-5)\n",
    "loss_fcn = nn.BCEWithLogitsLoss()\n",
    "train_loss = []\n",
    "epoch = 0\n",
    "train_correct = 0\n",
    "acc_loss = 0\n",
    "acc_avg = 0        \n",
    "\n",
    "model.train() #Set train mode\n",
    "with tqdm(total = len(valid_loader)) as epoch_pbar:                    \n",
    "    for i, batch in enumerate(valid_loader):\n",
    "        tokens = batch['tokens']\n",
    "        tokens['tokens'] = tokens['tokens'].to(device)\n",
    "        labels = batch['label'].to(device) \n",
    "\n",
    "        #Forward pass            \n",
    "        outputs = model(tokens)\n",
    "        loss = loss_fcn(outputs, labels)\n",
    "        acc_loss += loss.item()                \n",
    "        train_loss.append(loss.item())\n",
    "        logits = torch.sigmoid(outputs)\n",
    "        train_correct += torch.sum((logits >= .5) == labels).item()\n",
    "\n",
    "        #Update progress bar\n",
    "        avg_loss = acc_loss/(i + 1)                \n",
    "        acc_avg = train_correct/((i+1) * 16)\n",
    "        desc = f\"Epoch {epoch} - loss {avg_loss:.4f} - acc {acc_avg:.4f} - lr {optimizer.param_groups[0]['lr']}\"\n",
    "        epoch_pbar.set_description(desc)\n",
    "        epoch_pbar.update(1)\n",
    "\n",
    "        #Compute gradient and update params\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #clip gradient\n",
    "        optimizer.step()                \n",
    "        #scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()                     \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-3)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(train_loader.dataset))\n",
    "\n",
    "        \n",
    "for epoch in range(10):\n",
    "    ### TRAINING ###    \n",
    "    train_loss = []\n",
    "    train_correct = 0\n",
    "    acc_loss = 0\n",
    "    acc_avg = 0        \n",
    "    \n",
    "    model.train() #Set train mode\n",
    "    with tqdm(total = len(train_loader)) as epoch_pbar:                    \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            tokens = batch['tokens']\n",
    "            tokens['tokens'] = tokens['tokens'].to(device)\n",
    "            labels = batch['label'].to(device) \n",
    "\n",
    "            #Forward pass            \n",
    "            outputs = model(tokens)\n",
    "            loss = loss_fcn(outputs, labels)\n",
    "            acc_loss += loss.item()                \n",
    "            train_loss.append(loss.item())\n",
    "            logits = torch.sigmoid(outputs)\n",
    "            train_correct += torch.sum((logits >= .5) == labels).item()\n",
    "\n",
    "            #Update progress bar\n",
    "            avg_loss = acc_loss/(i + 1)                \n",
    "            acc_avg = train_correct/((i+1) * 16)\n",
    "            desc = f\"Epoch {epoch} - loss {avg_loss:.4f} - acc {acc_avg:.4f} - lr {optimizer.param_groups[0]['lr']}\"\n",
    "            epoch_pbar.set_description(desc)\n",
    "            epoch_pbar.update(1)\n",
    "\n",
    "            #Compute gradient and update params\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #clip gradient\n",
    "            optimizer.step()                \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, epoch_pbar, optimizer, scheduler):\n",
    "    train_loss = []\n",
    "    train_correct = 0\n",
    "    acc_loss =0\n",
    "    acc_avg = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        tokens = batch['tokens']\n",
    "        tokens['tokens'] = tokens['tokens'].to(device)\n",
    "        labels = batch['label'].to(device) \n",
    "    \n",
    "        #Forward pass            \n",
    "        outputs = model(tokens)\n",
    "        loss = loss_fcn(outputs, labels)\n",
    "        acc_loss += loss.item()                \n",
    "        train_loss.append(loss.item())\n",
    "        logits = torch.sigmoid(outputs)\n",
    "        train_correct += torch.sum((logits >= .5) == labels).item()\n",
    "        \n",
    "        #Update progress bar\n",
    "        avg_loss = acc_loss/(i + 1)                \n",
    "        acc_avg = train_correct/((i+1) * 16)\n",
    "        desc = f\"Epoch {epoch} - loss {avg_loss:.4f} - acc {acc_avg:.4f} - lr {optimizer.param_groups[0]['lr']}\"\n",
    "        epoch_pbar.set_description(desc)\n",
    "        epoch_pbar.update(1)\n",
    "        \n",
    "        #Compute gradient and update params\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #clip gradient\n",
    "        optimizer.step()                \n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "    \n",
    "    return train_loss, train_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss 0.4596 - acc 0.7956 - lr 0.0029712684315895224: 100%|██████████| 476/476 [00:53<00:00,  8.96it/s]\n",
      "Epoch 1 - loss 0.3927 - acc 0.8289 - lr 0.002885937723883112: 100%|██████████| 476/476 [00:53<00:00,  8.91it/s] \n",
      "Epoch 2 - loss 0.3633 - acc 0.8485 - lr 0.0027472903965443645: 100%|██████████| 476/476 [00:53<00:00,  8.98it/s]\n",
      "Epoch 3 - loss 0.3292 - acc 0.8606 - lr 0.002560660171779821: 100%|██████████| 476/476 [00:52<00:00,  9.01it/s] \n",
      "Epoch 4 - loss 0.3007 - acc 0.8738 - lr 0.0023332266598520487: 100%|██████████| 476/476 [00:52<00:00,  9.02it/s]\n",
      "Epoch 5 - loss 0.2750 - acc 0.8854 - lr 0.0020737391615688254: 100%|██████████| 476/476 [00:52<00:00,  9.01it/s]\n",
      "Epoch 6 - loss 0.2297 - acc 0.9076 - lr 0.0017921800852353012: 100%|██████████| 476/476 [00:52<00:00,  9.00it/s]\n",
      "Epoch 7 - loss 0.2010 - acc 0.9169 - lr 0.00149938092632073: 100%|██████████| 476/476 [00:52<00:00,  9.04it/s]  \n",
      "Epoch 8 - loss 0.1848 - acc 0.9303 - lr 0.0012066055829898698: 100%|██████████| 476/476 [00:52<00:00,  9.08it/s]\n",
      "Epoch 9 - loss 0.1608 - acc 0.9364 - lr 0.000925117037228928: 100%|██████████| 476/476 [00:52<00:00,  9.06it/s] \n"
     ]
    }
   ],
   "source": [
    "model = returnElmoModel()\n",
    "model.to(device)\n",
    "model.zero_grad()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-3)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(train_loader.dataset))\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    with tqdm(total = len(train_loader)) as epoch_pbar:                    \n",
    "        train_loss, train_correct = train_epoch(model, train_loader, epoch_pbar, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch(epoch, valid_size, model, device, valid_loader, epoch_pbar, \n",
    "                    optimizer, scheduler, writer, loss_fcn):\n",
    "    for i, batch in enumerate(valid_iter):\n",
    "        tokens = batch['tokens']\n",
    "        tokens['batch'] = tokens['tokens'].to(device)\n",
    "        labels = batch['label'].to(device) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens)\n",
    "\n",
    "        loss = loss_fcn(outputs, labels)\n",
    "        valid_loss.append(loss.item())\n",
    "        logits = torch.sigmoid(outputs)\n",
    "        predict = logits >= .5\n",
    "        valid_correct += torch.sum(predict == labels).item()\n",
    "\n",
    "        #Add to tensorboard\n",
    "        writer.add_scalar('Iteration Validation Loss', loss.item(), \n",
    "                          epoch*valid_size + i + 1)\n",
    "\n",
    "        for t, p in zip(labels, predict):\n",
    "            conf_matrix[t, p] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch(epoch, valid_size, model, device, valid_iter, epoch_pbar, \n",
    "                    optimizer, scheduler, writer, loss_fcn, conf_matrix):\n",
    "    valid_loss = []\n",
    "    valid_correct = 0\n",
    "        \n",
    "    for i, batch in enumerate(valid_iter):\n",
    "        tokens = batch['tokens']\n",
    "        tokens['tokens'] = tokens['tokens'].to(device)\n",
    "        labels = batch['label'].to(device)         \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens)\n",
    "\n",
    "        loss = loss_fcn(outputs, labels)\n",
    "        valid_loss.append(loss.item())\n",
    "        logits = torch.sigmoid(outputs)\n",
    "        predict = logits >= .5\n",
    "        valid_correct += torch.sum(predict == labels).item()\n",
    "\n",
    "        #Add to tensorboard\n",
    "        writer.add_scalar('Iteration Validation Loss', loss.item(), \n",
    "                          epoch*valid_size + i + 1)\n",
    "\n",
    "        import pdb;pdb.set_trace()\n",
    "        labels = labels.long()\n",
    "        predict = predict.long()\n",
    "        for t, p in zip(labels, predict):\n",
    "            conf_matrix[t, p] += 1            \n",
    "        \n",
    "    return valid_loss, valid_correct, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "conf_matrix = torch.zeros(2, 2)\n",
    "valid_iter = valid_loader\n",
    "valid_size = len(valid_loader)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(train_loader.dataset))\n",
    "writer = SummaryWriter(log_dir=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/102 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-28-1f30c43ae8de>(25)valid_one_epoch()\n",
      "-> labels = labels.long()\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(26)valid_one_epoch()\n",
      "-> predict = predict.long()\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(27)valid_one_epoch()\n",
      "-> for t, p in zip(labels, predict):\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(28)valid_one_epoch()\n",
      "-> conf_matrix[t, p] += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(27)valid_one_epoch()\n",
      "-> for t, p in zip(labels, predict):\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(28)valid_one_epoch()\n",
      "-> conf_matrix[t, p] += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(27)valid_one_epoch()\n",
      "-> for t, p in zip(labels, predict):\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(28)valid_one_epoch()\n",
      "-> conf_matrix[t, p] += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(27)valid_one_epoch()\n",
      "-> for t, p in zip(labels, predict):\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(28)valid_one_epoch()\n",
      "-> conf_matrix[t, p] += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(27)valid_one_epoch()\n",
      "-> for t, p in zip(labels, predict):\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(28)valid_one_epoch()\n",
      "-> conf_matrix[t, p] += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(27)valid_one_epoch()\n",
      "-> for t, p in zip(labels, predict):\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(28)valid_one_epoch()\n",
      "-> conf_matrix[t, p] += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(27)valid_one_epoch()\n",
      "-> for t, p in zip(labels, predict):\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(28)valid_one_epoch()\n",
      "-> conf_matrix[t, p] += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-28-1f30c43ae8de>(27)valid_one_epoch()\n",
      "-> for t, p in zip(labels, predict):\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total = valid_size) as epoch_pbar:\n",
    "    model.eval()                           \n",
    "    \n",
    "    valid_loss, valid_correct, conf_matrix = valid_one_epoch(epoch, valid_size, model, \n",
    "                                                             device, valid_iter, epoch_pbar, \n",
    "                                                             optimizer, scheduler, writer, \n",
    "                                                             loss_fcn, conf_matrix)                                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
